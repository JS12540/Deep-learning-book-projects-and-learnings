{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMr44VHpT8Ph",
        "outputId": "fec550c6-045d-4bb7-9490-9990dfeec0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9532k      0  0:00:08  0:00:08 --:--:-- 11.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "_QI4WOv9UAA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhnOa6gFUB2F",
        "outputId": "4d93e7cd-7d93-451c-abe5-196c9cfb7a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        " os.makedirs(val_dir / category)\n",
        " files = os.listdir(train_dir / category)\n",
        " random.Random(1337).shuffle(files) # Shuffle the list of training files using a seed, to ensure we get the same validation set every time we run the code.\n",
        " num_val_samples = int(0.2 * len(files))\n",
        " val_files = files[-num_val_samples:]\n",
        " for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname,val_dir / category / fname)"
      ],
      "metadata": {
        "id": "Gskojv3FUEAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        " \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        " \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        " \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2GM_ThWUHcb",
        "outputId": "1ab4b78b-dafc-4522-96de-f26021a00936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0VesyEKUJrb",
        "outputId": "cfb42bc9-650d-4ece-97fc-f7fa720d046d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'I think a great many viewers missed entirely the fact that this is obviously a parody of western films.<br /><br />This is not a bad movie - it is a clever tongue in cheek take on westerns. I don\\'t believe this film was taking itself seriously for a moment.<br /><br />What makes this film even more unique is the fact it is centered around 4 strong, beautiful women, two of which are black, one Asian, and a Mexican/Hispanic character.<br /><br />These aren\\'t your usual western women--they\\'re tough--they can draw fast and shoot straight.<br /><br />They\\'re so tough even the bartender is shaking when he pours their whiskey.<br /><br />The plot which moves this story along is typical of westerns--in the vein of \"you shot my brother--so I\\'m gonna get you!\" Only in this western, a woman\\'s sister has been shot and she\\'s out for vengeance on the gang who did it.<br /><br />So she goes and rounds up her old cronies from her bank robbing days.<br /><br />One of them, Maria, is not really all that interested in avenging Rachel\\'s sister, but she is motived by the fact there\\'s gold and jewelry hidden in the town where they\\'re headed.<br /><br />There are a couple of scenes that don\\'t quite make sense, not that they interfere that much, they can be ignored, but I wondered why they were there. So the film could use a little tightening, but over all, this is a well made film that has failed to find an audience that recognized what it is.<br /><br />My only disappointment was that the only lesbian in the film is a villain--of the \"heroines\", one is obviously straight, the others sexual orientations are never disclosed.<br /><br />7 Stars', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "jyTYP8A0Udqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the vocabulary to the 20,000 most frequent words. Otherwise we’d be indexing every word in the training data— potentially tens of thousands of terms that only occur once or\n",
        "# twice and thus aren’t informative. In general, 20,000 is the right vocabulary size for text classification.\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        " max_tokens=20000,\n",
        " output_mode=\"multi_hot\", # Encode the output tokens as multi-hot binary vectors.\n",
        ")"
      ],
      "metadata": {
        "id": "LQaeFwqxU541"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_only_train_ds = train_ds.map(lambda x, y: x) # Prepare a dataset that only yields raw text inputs (no labels)."
      ],
      "metadata": {
        "id": "TKE0yY3FUf6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "F9NEgbLCUitu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        " max_tokens=max_tokens,\n",
        " output_mode=\"int\",\n",
        " output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "4JGIMknvUqHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_train_ds = train_ds.map(\n",
        " lambda x, y: (text_vectorization(x), y),\n",
        " num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        " lambda x, y: (text_vectorization(x), y),\n",
        " num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        " lambda x, y: (text_vectorization(x), y),\n",
        " num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "_e_hZQ3YUsYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Understanding self-attention"
      ],
      "metadata": {
        "id": "G476g5s3nXwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you’re going through this book, you may be skimming some parts and attentively\n",
        "reading others, depending on what your goals or interests are.\n",
        "\n",
        "What if your models\n",
        "did the same? It’s a simple yet powerful idea: not all input information seen by a\n",
        "model is equally important to the task at hand, so models should “pay more attention”\n",
        "to some features and “pay less attention” to other features."
      ],
      "metadata": {
        "id": "R-2fvXBhnaYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does that sound familiar? You’ve already encountered a similar concept twice in\n",
        "this book:\n",
        "\n",
        "\n",
        " Max pooling in convnets looks at a pool of features in a spatial region and\n",
        "selects just one feature to keep. That’s an “all or nothing” form of attention:\n",
        "keep the most important feature and discard the rest.\n",
        "\n",
        "\n",
        " TF-IDF normalization assigns importance scores to tokens based on how much\n",
        "information different tokens are likely to carry. Important tokens get boosted\n",
        "while irrelevant tokens get faded out. That’s a continuous form of attention."
      ],
      "metadata": {
        "id": "1lpphirFnerH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many different forms of attention you could imagine, but they all start by computing importance scores for a set of features, with higher scores for more relevant features and lower scores for less relevant ones.\n",
        "\n",
        " How these scores should be\n",
        "computed, and what you should do with them, will vary from approach to approach."
      ],
      "metadata": {
        "id": "m1IJH6NwnkTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crucially, this kind of attention mechanism can be used for more than just highlighting\n",
        "or erasing certain features. It can be used to make features context-aware. You’ve just\n",
        "learned about word embeddings—vector spaces that capture the “shape” of the semantic\n",
        "relationships between different words. In an embedding space, a single word has a fixed\n",
        "position—a fixed set of relationships with every other word in the space"
      ],
      "metadata": {
        "id": "1GEzHOVinsSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But that’s not\n",
        "quite how language works: the meaning of a word is usually context-specific. When you\n",
        "mark the date, you’re not talking about the same “date” as when you go on a date, nor is\n",
        "it the kind of date you’d buy at the market. When you say, “I’ll see you soon,” the meaning of the word “see” is subtly different from the “see” in “I’ll see this project to its end” or\n",
        "“I see what you mean.” And, of course, the meaning of pronouns like “he,” “it,” “in,” etc.,\n",
        "is entirely sentence-specific and can even change multiple times within a single sentence."
      ],
      "metadata": {
        "id": "Jkasj7E9ns1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, a smart embedding space would provide a different vector representation\n",
        "for a word depending on the other words surrounding it. That’s where self-attention\n",
        "comes in. **The purpose of self-attention is to modulate the representation of a token**\n",
        "**by using the representations of related tokens in the sequence**. This produces contextaware token representations"
      ],
      "metadata": {
        "id": "GPvPE2VJnu1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[How self attention works and how attention scores are calculated](https://chat.openai.com/share/b2b4096d-5756-4084-9b55-ef28c9d52a51)"
      ],
      "metadata": {
        "id": "R4y3uB67oAn0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJa9jpFyRr3I"
      },
      "outputs": [],
      "source": [
        "def self_attention(input_sequence):\n",
        " output = np.zeros(shape=input_sequence.shape)\n",
        " for i, pivot_vector in enumerate(input_sequence): # Iterate over each token in the input sequence\n",
        "    scores = np.zeros(shape=(len(input_sequence),))\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      scores[j] = np.dot(pivot_vector, vector.T) # Compute the dot product (attention score) between the token and every other token.\n",
        "    # Scale by a normalization factor, and apply a softmax.\n",
        "    scores /= np.sqrt(input_sequence.shape[1])\n",
        "    scores = softmax(scores)\n",
        "    new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
        "    for j, vector in enumerate(input_sequence):\n",
        "      new_pivot_representation += vector * scores[j] # Take the sum of all tokens weighted by the attention scores\n",
        "    output[i] = new_pivot_representation # That sum is our output.\n",
        " return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Understanding self Attention and Multi Head Self Attention](https://chat.openai.com/share/cac3cb92-fcf4-4c5c-801a-bc609ec4689e)"
      ],
      "metadata": {
        "id": "x1_YlESUp5KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original Transformer architecture consists of two parts: a Transformer encoder that\n",
        "processes the source sequence, and a Transformer decoder that uses the source sequence\n",
        "to generate a translated version. You’ll learn about about the decoder part in a minute.\n",
        "\n",
        "\n",
        " Crucially, the encoder part can be used for text classification—it’s a very generic\n",
        "module that ingests a sequence and learns to turn it into a more useful representation. Let’s implement a Transformer encoder and try it on the movie review sentiment\n",
        "classification task."
      ],
      "metadata": {
        "id": "1a4VowSoqB3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Transformer encoder implemented as a subclassed Layer"
      ],
      "metadata": {
        "id": "6JYu8GJUqGtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "YWsyvLsfp-5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim # Size of the input token vectors\n",
        "    self.dense_dim = dense_dim # Size of the inner dense layer\n",
        "    self.num_heads = num_heads # Number of attention heads\n",
        "    self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "        layers.Dense(embed_dim),]\n",
        "        )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "  def call(self, inputs, mask=None): # Computation goes in call().\n",
        "    if mask is not None: # The mask that will be generated by the Embedding layer will be 2D, but the attention layer expects to be 3D or 4D, so we expand its rank.\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(\n",
        "    inputs, inputs, attention_mask=mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "  def get_config(self): # Implement serialization so we can save the model.\n",
        "    config = super().get_config()\n",
        "    config.update({\"embed_dim\": self.embed_dim,\"num_heads\": self.num_heads,\"dense_dim\": self.dense_dim,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "o6SmXk53qLFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’ll note that the normalization layers we’re using here aren’t BatchNormalization\n",
        "layers like those we’ve used before in image models. That’s because BatchNormalization\n",
        "doesn’t work well for sequence data. Instead, we’re using the LayerNormalization layer,\n",
        "which normalizes each sequence independently from other sequences in the batch.\n",
        "Like this, in NumPy-like pseudocode:"
      ],
      "metadata": {
        "id": "U8x3_dKOuhB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_normalization(batch_of_sequences): # Input shape: (batch_size, sequence_length, embedding_dim)\n",
        "\n",
        " # To compute mean and variance, we only pool data over the last axis (axis -1).\n",
        " mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n",
        " variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n",
        " return (batch_of_sequences - mean) / variance"
      ],
      "metadata": {
        "id": "X3sdj4Mpui5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare to BatchNormalization (during training):"
      ],
      "metadata": {
        "id": "rXTg1wrsuvLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_normalization(batch_of_images): # Input shape: (batch_size, height, width, channels)\n",
        "\n",
        " # Pool data over the batch axis (axis 0), which creates interactions between samples in a batch.\n",
        " mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        " variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        " return (batch_of_images - mean) / variance"
      ],
      "metadata": {
        "id": "tkUEhCA4uvqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While BatchNormalization collects information from many samples to obtain accurate statistics for the feature means and variances, LayerNormalization pools data\n",
        "within each sequence separately, which is more appropriate for sequence data."
      ],
      "metadata": {
        "id": "hPkZ-VguvKxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now that we’ve implemented our TransformerEncoder, we can use it to assemble a\n",
        "text-classification model similar to the GRU-based one you’ve seen previously."
      ],
      "metadata": {
        "id": "nw4kazIAvLfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x) # Since TransformerEncoder returns full sequences, we need to reduce each sequence to a single vector for classification via a global pooling layer.\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "INFsywuBvNmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the TransformerEncoder returns a sequence, the GlobalMaxPooling1D layer is used to reduce each sequence to a single vector. This is a common technique to convert variable-length sequences into fixed-size representations that can be used for classification."
      ],
      "metadata": {
        "id": "88HlQy60Vv5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQK4tBy_VMXa",
        "outputId": "ab369759-3874-421a-975e-a41625670c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5664033 (21.61 MB)\n",
            "Trainable params: 5664033 (21.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        " keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        " save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20,\n",
        " callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73U1MzhyWvYa",
        "outputId": "79fa80df-8bf0-4b5b-9ce8-29c7d2293ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 59s 87ms/step - loss: 0.5003 - accuracy: 0.7618 - val_loss: 0.4032 - val_accuracy: 0.8134\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 48s 77ms/step - loss: 0.3477 - accuracy: 0.8497 - val_loss: 0.3155 - val_accuracy: 0.8666\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.3128 - accuracy: 0.8669 - val_loss: 0.3161 - val_accuracy: 0.8626\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.2853 - accuracy: 0.8789 - val_loss: 0.3221 - val_accuracy: 0.8606\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.2626 - accuracy: 0.8922 - val_loss: 0.2889 - val_accuracy: 0.8756\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.2374 - accuracy: 0.9038 - val_loss: 0.3124 - val_accuracy: 0.8718\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 43s 70ms/step - loss: 0.2086 - accuracy: 0.9176 - val_loss: 0.2858 - val_accuracy: 0.8802\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.1837 - accuracy: 0.9301 - val_loss: 0.3065 - val_accuracy: 0.8742\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.1591 - accuracy: 0.9413 - val_loss: 0.3015 - val_accuracy: 0.8812\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.1296 - accuracy: 0.9541 - val_loss: 0.3605 - val_accuracy: 0.8660\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.1126 - accuracy: 0.9590 - val_loss: 0.3463 - val_accuracy: 0.8702\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0892 - accuracy: 0.9696 - val_loss: 0.3467 - val_accuracy: 0.8730\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0708 - accuracy: 0.9757 - val_loss: 0.4323 - val_accuracy: 0.8632\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.0554 - accuracy: 0.9821 - val_loss: 0.4834 - val_accuracy: 0.8630\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.0418 - accuracy: 0.9853 - val_loss: 0.4959 - val_accuracy: 0.8658\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0348 - accuracy: 0.9880 - val_loss: 0.5366 - val_accuracy: 0.8662\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 43s 68ms/step - loss: 0.0302 - accuracy: 0.9905 - val_loss: 0.5045 - val_accuracy: 0.8656\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 0.5817 - val_accuracy: 0.8692\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 42s 67ms/step - loss: 0.0211 - accuracy: 0.9927 - val_loss: 0.6302 - val_accuracy: 0.8624\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 43s 69ms/step - loss: 0.0180 - accuracy: 0.9936 - val_loss: 0.5986 - val_accuracy: 0.8672\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79d3783430d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\n",
        " \"transformer_encoder.keras\",\n",
        " custom_objects={\"TransformerEncoder\": TransformerEncoder}) # Provide the custom TransformerEncode  class to the model-loading process.\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e1hooCCWzJc",
        "outputId": "4efd3a81-289e-44b1-fffd-e2462a152ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 19s 23ms/step - loss: 0.3074 - accuracy: 0.8735\n",
            "Test acc: 0.874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer was a hybrid approach that is technically order-agnostic, but that manually\n",
        "injects order information in the representations it processes. This is the missing ingredient! It’s called positional encoding"
      ],
      "metadata": {
        "id": "3VxUBboBZlxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USING POSITIONAL ENCODING TO RE-INJECT ORDER INFORMATION"
      ],
      "metadata": {
        "id": "5ArgrX_KZmr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind positional encoding is very simple: to give the model access to wordorder information, we’re going to add the word’s position in the sentence to each word\n",
        "embedding. Our input word embeddings will have two components: the usual word vector, which represents the word independently of any specific context, and a position vector, which represents the position of the word in the current sentence. Hopefully, the model will then figure out how to best leverage this additional information."
      ],
      "metadata": {
        "id": "4rewtD05dXzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original “Attention is all you need” paper used an interesting trick to encode\n",
        "word positions: it added to the word embeddings a vector containing values in the\n",
        "range [-1, 1] that varied cyclically depending on the position (it used cosine functions to achieve this). This trick offers a way to uniquely characterize any integer in a\n",
        "large range via a vector of small values. It’s clever, but it’s not what we’re going to use\n",
        "in our case. We’ll do something simpler and more effective: we’ll learn positionembedding vectors the same way we learn to embed word indices. We’ll then proceed\n",
        "to add our position embeddings to the corresponding word embeddings, to obtain a\n",
        "position-aware word embedding. This technique is called “positional embedding.”"
      ],
      "metadata": {
        "id": "XbgGvfrUdcaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        " def __init__(self, sequence_length, input_dim, output_dim, **kwargs): # A downside of position embeddings is that the sequence length needs to be known in advance\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim) # Prepare an Embedding layer for the token indices.\n",
        "    self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim) # And another one for the token positions\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        " def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1) # delta=1: The step size between consecutive values in the sequence is 1.\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions # Add both embedding vectors together.\n",
        "\n",
        " def compute_mask(self, inputs, mask=None):\n",
        "    # Like the Embedding layer, this layer should be able to generate a mask so we can ignore padding 0s in the inputs. The compute_mask method will called automatically by the framework, and the mask will get propagated\n",
        "    # to the next layer.\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        " def get_config(self):\n",
        "    # Implement serialization so we can save the model.\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "      \"output_dim\": self.output_dim,\n",
        "      \"sequence_length\": self.sequence_length,\n",
        "      \"input_dim\": self.input_dim,\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "IBHbUASjZpdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You would use this PositionEmbedding layer just like a regular Embedding layer"
      ],
      "metadata": {
        "id": "h112pmrqea-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUTTING IT ALL TOGETHER: A TEXT-CLASSIFICATION TRANSFORMER"
      ],
      "metadata": {
        "id": "tJlYplkifbid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All you have to do to start taking word order into account is swap the old Embedding\n",
        "layer with our position-aware version"
      ],
      "metadata": {
        "id": "tqX60HYHffkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        " loss=\"binary_crossentropy\",\n",
        " metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UWFcyOPebs2",
        "outputId": "065a622b-3119-43f6-ae35-e237fe593283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " positional_embedding (Posi  (None, None, 256)         5273600   \n",
            " tionalEmbedding)                                                \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tra  (None, None, 256)         543776    \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5817633 (22.19 MB)\n",
            "Trainable params: 5817633 (22.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        " keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        " save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20,\n",
        "callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-F2rVQtfrwr",
        "outputId": "302a8782-f5f4-4fa0-a92c-f73eb0856169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 61s 93ms/step - loss: 0.5391 - accuracy: 0.7395 - val_loss: 0.3223 - val_accuracy: 0.8532\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 51s 81ms/step - loss: 0.2979 - accuracy: 0.8748 - val_loss: 0.3500 - val_accuracy: 0.8718\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 46s 74ms/step - loss: 0.2384 - accuracy: 0.9050 - val_loss: 0.2759 - val_accuracy: 0.8886\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.1967 - accuracy: 0.9232 - val_loss: 0.3200 - val_accuracy: 0.8790\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.1647 - accuracy: 0.9359 - val_loss: 0.4637 - val_accuracy: 0.8686\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.1415 - accuracy: 0.9459 - val_loss: 0.4335 - val_accuracy: 0.8796\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 45s 73ms/step - loss: 0.1199 - accuracy: 0.9559 - val_loss: 0.5659 - val_accuracy: 0.8738\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.1050 - accuracy: 0.9628 - val_loss: 0.4942 - val_accuracy: 0.8792\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 46s 73ms/step - loss: 0.0832 - accuracy: 0.9697 - val_loss: 0.5833 - val_accuracy: 0.8842\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 45s 71ms/step - loss: 0.0690 - accuracy: 0.9769 - val_loss: 0.5047 - val_accuracy: 0.8662\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0573 - accuracy: 0.9810 - val_loss: 0.5453 - val_accuracy: 0.8754\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0412 - accuracy: 0.9859 - val_loss: 0.5329 - val_accuracy: 0.8784\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0352 - accuracy: 0.9880 - val_loss: 0.6367 - val_accuracy: 0.8730\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0286 - accuracy: 0.9898 - val_loss: 0.8426 - val_accuracy: 0.8654\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0271 - accuracy: 0.9919 - val_loss: 0.7897 - val_accuracy: 0.8760\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 45s 72ms/step - loss: 0.0245 - accuracy: 0.9931 - val_loss: 0.7181 - val_accuracy: 0.8724\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0179 - accuracy: 0.9945 - val_loss: 0.9293 - val_accuracy: 0.8710\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 44s 71ms/step - loss: 0.0153 - accuracy: 0.9954 - val_loss: 1.0993 - val_accuracy: 0.8738\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0148 - accuracy: 0.9952 - val_loss: 1.0026 - val_accuracy: 0.8770\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 44s 70ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.9356 - val_accuracy: 0.8712\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79d378341960>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\n",
        " \"full_transformer_encoder.keras\",\n",
        " custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        " \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd9CBQZmfvGy",
        "outputId": "439dc35e-d685-4de5-f876-1b0ce4058413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 22s 28ms/step - loss: 0.2905 - accuracy: 0.8792\n",
            "Test acc: 0.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get to 88.3% test accuracy, a solid improvement that clearly demonstrates the\n",
        "value of word order information for text classification. This is our best sequence\n",
        "model so far! However, it’s still one notch below the bag-of-words approach."
      ],
      "metadata": {
        "id": "cXxF9aqHf6bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  When to use sequence models over bag-of-words models"
      ],
      "metadata": {
        "id": "A0MhElPghuqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may sometimes hear that bag-of-words methods are outdated, and that Transformerbased sequence models are the way to go, no matter what task or dataset you’re looking at. This is definitely not the case: a small stack of Dense layers on top of a bag-ofbigrams remains a perfectly valid and relevant approach in many cases. In fact, among\n",
        "the various techniques that we’ve tried on the IMDB dataset throughout this chapter,\n",
        "the best performing so far was the bag-of-bigrams!"
      ],
      "metadata": {
        "id": "3CIXB0cHhvkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In 2017, my team and I ran a systematic analysis of the performance of various textclassification techniques across many different types of text datasets, and we discovered a remarkable and surprising rule of thumb for deciding whether to go with a\n",
        "bag-of-words model or a sequence model (http://mng.bz/AOzK)     —a golden constant\n",
        "of sorts.   "
      ],
      "metadata": {
        "id": "zc-Exw1ih1fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It turns out that when approaching a new text-classification task, you should pay\n",
        "close attention to the ratio between the number of samples in your training data and\n",
        "the mean number of words per sample . If that ratio is small—less\n",
        "than 1,500—then the bag-of-bigrams model will perform better (and as a bonus, it will\n",
        "be much faster to train and to iterate on too). If that ratio is higher than 1,500, then\n",
        "you should go with a sequence model. In other words, sequence models work best\n",
        "when lots of training data is available and when each sample is relatively short."
      ],
      "metadata": {
        "id": "Q9r9bdASiZnu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_7lAbW9Jf7Cr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}