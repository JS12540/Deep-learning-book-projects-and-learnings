{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ba7VXtF6VukF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a supervised-learning training step ends up\n",
        "looking like this:"
      ],
      "metadata": {
        "id": "gYZfZsQkUF2J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SDKk0OuNPsVV"
      },
      "outputs": [],
      "source": [
        "def train_step(inputs, targets):\n",
        " with tf.GradientTape() as tape:\n",
        "  predictions = model(inputs, training=True)\n",
        "  loss = loss_fn(targets, predictions)\n",
        " gradients = tape.gradients(loss, model.trainable_weights)\n",
        " optimizer.apply_gradients(zip(model.trainable_weights, gradients))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "def get_mnist_model():\n",
        " inputs = keras.Input(shape=(28 * 28,))\n",
        " features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        " features = layers.Dropout(0.5)(features)\n",
        " outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        " model = keras.Model(inputs, outputs)\n",
        " return model"
      ],
      "metadata": {
        "id": "Mg0KJTviV1y6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqnqCDc7V4Gs",
        "outputId": "b93deebd-d3b2-495e-d60b-0555c2c037b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_mnist_model()\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "loss_tracking_metric = keras.metrics.Mean() # Prepare a Mean metric tracker to keep track of the loss average"
      ],
      "metadata": {
        "id": "w-ZhO0VPWdBY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(inputs, targets):\n",
        " with tf.GradientTape() as tape:\n",
        "  predictions = model(inputs, training=True) # Run the forward pass. Note that we pass training=True.\n",
        "  loss = loss_fn(targets, predictions)\n",
        " gradients = tape.gradient(loss, model.trainable_weights) # Run the backward pass. Note that we use model.trainable_weights.\n",
        " optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        " logs = {}\n",
        " for metric in metrics: # Keep track of metrics.\n",
        "  metric.update_state(targets, predictions)\n",
        "  logs[metric.name] = metric.result()\n",
        " loss_tracking_metric.update_state(loss)   # Keep track of the loss average.\n",
        " logs[\"loss\"] = loss_tracking_metric.result()\n",
        " return logs"
      ],
      "metadata": {
        "id": "tlKXigQOWpya"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to reset the state of our metrics at the start of each epoch and before running evaluation. Here’s a utility function to do it."
      ],
      "metadata": {
        "id": "JI2dLcP1XS8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_metrics():\n",
        " for metric in metrics:\n",
        "  metric.reset_state()\n",
        " loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "71wIPGuxXToa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now lay out our complete training loop. Note that we use a tf.data.Dataset\n",
        "object to turn our NumPy data into an iterator that iterates over the data in batches of\n",
        "size 32."
      ],
      "metadata": {
        "id": "aJv63aHtXZtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        " reset_metrics()\n",
        " for inputs_batch, targets_batch in training_dataset:\n",
        "  logs = train_step(inputs_batch, targets_batch)\n",
        " print(f\"Results at the end of epoch {epoch}\")\n",
        " for key, value in logs.items():\n",
        "  print(f\"...{key}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5r_kEawXaJf",
        "outputId": "b908e56a-dfe3-4199-8eea-d098db4c6422"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results at the end of epoch 0\n",
            "...sparse_categorical_accuracy: 0.9132\n",
            "...loss: 0.2899\n",
            "Results at the end of epoch 1\n",
            "...sparse_categorical_accuracy: 0.9532\n",
            "...loss: 0.1602\n",
            "Results at the end of epoch 2\n",
            "...sparse_categorical_accuracy: 0.9633\n",
            "...loss: 0.1274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here’s the evaluation loop: a simple for loop that repeatedly calls a test_step()\n",
        "function, which processes a single batch of data. The test_step() function is just a subset of the logic of train_step(). It omits the code that deals with updating the weights\n",
        "of the model—that is to say, everything involving the GradientTape and the optimizer."
      ],
      "metadata": {
        "id": "2GrQFoFlXr2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(inputs, targets):\n",
        " predictions = model(inputs, training=False) #Note that we pass training=False.\n",
        " loss = loss_fn(targets, predictions)\n",
        " logs = {}\n",
        " for metric in metrics:\n",
        "  metric.update_state(targets, predictions)\n",
        "  logs[\"val_\" + metric.name] = metric.result()\n",
        " loss_tracking_metric.update_state(loss)\n",
        " logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        " return logs"
      ],
      "metadata": {
        "id": "qY644OoiXsi2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        " logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"Evaluation results:\")\n",
        "for key, value in logs.items():\n",
        " print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8RPwK8cYBgE",
        "outputId": "0f6f6963-d3f2-4c8b-a401-61d7e13cb067"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9688\n",
            "...val_loss: 0.1194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if you need a custom training algorithm, but you still want to leverage the\n",
        "power of the built-in Keras training logic? There’s actually a middle ground between\n",
        "fit() and a training loop written from scratch: you can provide a custom training\n",
        "step function and let the framework do the rest.\n",
        "\n",
        " You can do this by overriding the train_step() method of the Model class. This is\n",
        "the function that is called by fit() for every batch of data. You will then be able to call\n",
        "fit() as usual, and it will be running your own learning algorithm under the hood."
      ],
      "metadata": {
        "id": "YubHy88Fb8cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a simple example:\n",
        "\n",
        " We create a new class that subclasses keras.Model.\n",
        "\n",
        " We override the method train_step(self, data). Its contents are nearly identical to what we used in the previous section. It returns a dictionary mapping\n",
        "metric names (including the loss) to their current values.\n",
        "\n",
        " We implement a metrics property that tracks the model’s Metric instances.\n",
        "This enables the model to automatically call reset_state() on the model’s\n",
        "metrics at the start of each epoch and at the start of a call to evaluate(), so you\n",
        "don’t have to do it by hand."
      ],
      "metadata": {
        "id": "mf05l92JcFvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This metric object will be used to track the average of per-batch losses during training and evaluation.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n"
      ],
      "metadata": {
        "id": "yAYrmE-1b_F3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(keras.Model):\n",
        " def train_step(self, data): # We override the train_step method.\n",
        "  inputs, targets = data\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = self(inputs, training=True) # We use self(inputs, training=True) instead of model(inputs,training=True), since our model is the class itself.\n",
        "    loss = loss_fn(targets, predictions)\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  self.optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "  loss_tracker.update_state(loss)\n",
        "  return {\"loss\": loss_tracker.result()}\n",
        " @property\n",
        " def metrics(self):  # Any metric you would like to reset across epochs should be listed here.\n",
        "  return [loss_tracker]"
      ],
      "metadata": {
        "id": "3ZUbnDz9dNqM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now instantiate our custom model, compile it (we only pass the optimizer, since\n",
        "the loss is already defined outside of the model), and train it using fit() as usual"
      ],
      "metadata": {
        "id": "v0VBNK2ieEBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_mnist_model()"
      ],
      "metadata": {
        "id": "3Y8PYo24ekRg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)"
      ],
      "metadata": {
        "id": "JefmkffoeEg-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop())\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyWY9vXAeQWl",
        "outputId": "0a0c8abc-814a-4382-d0af-1a0c9968427d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.2965\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1612\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1316\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79dab46929e0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " After\n",
        "you’ve called compile(), you get access to the following:\n",
        "\n",
        " self.compiled_loss—The loss function you passed to compile().\n",
        "\n",
        " self.compiled_metrics—A wrapper for the list of metrics you passed, which\n",
        "allows you to call self.compiled_metrics.update_state() to update all of\n",
        "your metrics at once.\n",
        "\n",
        " self.metrics—The actual list of metrics you passed to compile(). Note that it\n",
        "also includes a metric that tracks the loss, similar to what we did manually with\n",
        "our loss_tracking_metric earlier.\n",
        "\n",
        "We can thus write"
      ],
      "metadata": {
        "id": "9U7iQnDmgwp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(keras.Model):\n",
        " def train_step(self, data):\n",
        "  inputs, targets = data\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = self(inputs, training=True)\n",
        "    loss = self.compiled_loss(targets, predictions) # Compute the loss via self.compiled_loss.\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  self.optimizer.apply_gradients(zip(gradients, model.trainable_weights)) # By using self.optimizer instead of the global optimizer variable, you ensure that the optimizer is aware of the variables within your custom model.\n",
        "  self.compiled_metrics.update_state(targets, predictions)  # Update the model’s metrics via self.compiled_metrics.\n",
        "  return {m.name: m.result() for m in self.metrics} # Return a dict mapping metric names to their current value."
      ],
      "metadata": {
        "id": "3dE_g5dCg1QM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
        " loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        " metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5ZX32mUhQj7",
        "outputId": "1bec0548-8278-4531-d5e1-76d6d2ad28c3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2959 - sparse_categorical_accuracy: 0.9127\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9537\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.1334 - sparse_categorical_accuracy: 0.9633\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79dab473d3c0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}